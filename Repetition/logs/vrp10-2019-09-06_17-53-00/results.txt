actor_net_lr: 0.0001
agent_type: attention
batch_size: 128
beam_width: 10
capacity: 20
critic_net_lr: 0.0001
data_dir: data
decode_len: 16
demand_max: 9
disable_tqdm: True
dropout: 0.1
embedding_dim: 128
entropy_coedd: 0.0
forget_bias: 1.0
gpu: 0
hidden_dim: 128
infer_type: batch
input_dim: 3
is_train: True
load_path: 
log_dir: logs/vrp10-2019-09-06_17-53-00
log_interval: 200
mask_glimpses: True
mask_pointer: True
max_grad_norm: 2.0
model_dir: logs/vrp10-2019-09-06_17-53-00\model
n_cust: 10
n_glimpses: 0
n_nodes: 11
n_process_blocks: 3
n_train: 260
random_seed: 24601
rnn_layers: 1
save_interval: 1000
stdout_print: True
tanh_exploration: 10
task: vrp10
task_name: vrp
test_interval: 200
test_size: 100
use_tanh: False
随机数种子为:  24601
花费 51.87828874588013秒 建立代理.
训练开始
训练步数: 0 -- 时间: 00:00:24 -- 训练奖励: 7.441123008728027 -- 值: -0.11673089861869812
    actor loss: -154.2104034423828 -- critic loss: 59.52788543701172
在批处理模式中 greedy 的平均值: 6.566102504730225 -- std 1.288569688796997 -- time 5.145242691040039 s
在批处理模式中 beam_search 的平均值: 6.110528945922852 -- std 1.0411568880081177 -- time 2.473385810852051 s
##################################################################
训练步数: 200 -- 时间: 00:00:33 -- 训练奖励: 7.218361854553223 -- 值: 6.085821151733398
    actor loss: -18.817556381225586 -- critic loss: 3.398611068725586
在批处理模式中 greedy 的平均值: 6.5439863204956055 -- std 1.1515713930130005 -- time 0.40392088890075684 s
在批处理模式中 beam_search 的平均值: 6.021841526031494 -- std 0.9749063849449158 -- time 0.5944106578826904 s
##################################################################
总时间为： 00:01:07
