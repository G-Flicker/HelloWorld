actor_net_lr: 0.0001
agent_type: attention
batch_size: 128
beam_width: 10
capacity: 20
critic_net_lr: 0.0001
data_dir: data
decode_len: 16
demand_max: 9
disable_tqdm: True
dropout: 0.1
embedding_dim: 128
entropy_coedd: 0.0
forget_bias: 1.0
gpu: 0
hidden_dim: 128
infer_type: batch
input_dim: 3
is_train: True
load_path: 
log_dir: logs/vrp10-2019-09-06_17-29-34
log_interval: 200
mask_glimpses: True
mask_pointer: True
max_grad_norm: 2.0
model_dir: logs/vrp10-2019-09-06_17-29-34\model
n_cust: 10
n_glimpses: 0
n_nodes: 11
n_process_blocks: 3
n_train: 260
random_seed: 24601
rnn_layers: 1
save_interval: 1000
stdout_print: True
tanh_exploration: 10
task: vrp10
task_name: vrp
test_interval: 200
test_size: 100
use_tanh: False
随机数种子为:  24601
花费 45.83299517631531秒 建立代理.
训练开始
训练步数: 0 -- 时间: 00:00:21 -- 训练奖励: 7.765223979949951 -- 值: 0.1308557391166687
    actor loss: -159.89163208007812 -- critic loss: 60.517948150634766
在批处理模式中 greedy 的平均值: 8.150651931762695 -- std 1.9835177659988403 -- time 4.748883008956909 s
在批处理模式中 beam_search 的平均值: 7.738250255584717 -- std 1.6759854555130005 -- time 2.3119568824768066 s
##################################################################
训练步数: 200 -- 时间: 00:00:31 -- 训练奖励: 7.053403854370117 -- 值: 6.964328765869141
    actor loss: -1.826483130455017 -- critic loss: 1.8710832595825195
在批处理模式中 greedy 的平均值: 6.641357421875 -- std 1.0824419260025024 -- time 0.7342040538787842 s
在批处理模式中 beam_search 的平均值: 6.140125751495361 -- std 1.0124218463897705 -- time 0.6092319488525391 s
##################################################################
总时间为： 00:01:01
